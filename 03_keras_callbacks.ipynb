{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp keras_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Callbacks\n",
    "\n",
    "> Custom callbacks for tensorflow.keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from operator import gt, lt\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, \n",
    "    ModelCheckpoint, TensorBoard, LambdaCallback)\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CosineLRScheduler(Callback):\n",
    "    \"\"\"```Learning rate scheduler that makes updates each batch. Built-in\n",
    "    Keras version only updates once per epoch.\n",
    "    ```\"\"\"\n",
    "    \n",
    "    def __init__(self, max_lr, epochs, iters_per_epoch, warm=0.3, \n",
    "                 restarts=False, cycle_len=5, cycle_decay=0.0, min_lr=None, \n",
    "                 verbose=False):\n",
    "        \"\"\"```\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_lr: float\n",
    "            Maximum learning rate to use during training.\n",
    "        epochs: int\n",
    "            Number of epochs to train for.\n",
    "        iters_per_epoch: int\n",
    "            Number of batches in each epoch.\n",
    "        warm: float\n",
    "            Percent of training run (or cycle length) devoted to the increasing\n",
    "            portion of the schedule. Default 0.3.\n",
    "        restarts: bool\n",
    "            Specifies whether to use restarts, i.e. use a cyclical LR.\n",
    "            True: Version of cosine annealing with restarts. In one\n",
    "                  cycle, LR starts high and gradually decreases.\n",
    "                  At the start of the next cycle, it is \n",
    "                  immediately increased again.\n",
    "            False: Version of cosine annealing where LR increases\n",
    "                   for first 30% of training, then decreases for \n",
    "                   remaining 70%.          \n",
    "        cycle_len: int\n",
    "            Number of epochs contained in a single cycle. Only used\n",
    "            when scheduler uses restarts.\n",
    "        cycle_decay: float\n",
    "            Scalar to decay the learning rate at the end of each cycle.\n",
    "            This is only used with restarts, since the regular cosine \n",
    "            annealing already decays the LR over time.\n",
    "            E.g. 1.0 will use no decay. \n",
    "            0.9 means that cycle 2 LRs = cycle 1 LRs * 0.9, \n",
    "            cycle 3 LRs = cycle 1 LRs * .81,\n",
    "            etc.\n",
    "        min_lr: float\n",
    "            Minimum learning rate. If None is specified, it will be set\n",
    "            to max_lr / 25.\n",
    "        ```\"\"\"\n",
    "        super().__init__()\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr or max_lr / 25\n",
    "        self.epochs = epochs\n",
    "        self.iters_per_epoch = iters_per_epoch\n",
    "        self.iters = epochs * iters_per_epoch\n",
    "        self.warm = warm\n",
    "\n",
    "        if restarts and cycle_len < self.iters:\n",
    "            warnings.warn('Training will consist of less than 1 full cycle.')\n",
    "        \n",
    "        self.cycle_len = cycle_len\n",
    "        self.cycle_decay = cycle_decay\n",
    "        self.restarts = restarts\n",
    "        self.lrs = self._schedule(restarts)\n",
    "        self.curr_iter = 0\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Update learning rate based on current iteration.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'\\nUpdate LR from {self.model.optimizer.lr.numpy():.3E}', \n",
    "                  end='')\n",
    "        \n",
    "        # Continue using min_lr if training past specified max iters.\n",
    "        try:\n",
    "            lr = self.lrs[self.curr_iter]\n",
    "        except IndexError:\n",
    "            lr = self.lrs[-1]\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "        self.curr_iter += 1\n",
    "        if self.verbose:\n",
    "            print(f' to {self.model.optimizer.lr.numpy():.3E}')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _cosine_anneal(iters, lr1, lr2):\n",
    "        \"\"\"Helper function for _cosine_tri_lr(). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        iters: int\n",
    "            Number of iterations in segment.\n",
    "        lr1: float\n",
    "            Learning rate at start of segment.\n",
    "        lr2: float\n",
    "            Learning rate at end of segment.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "        \"\"\"\n",
    "        i = np.arange(iters)\n",
    "        return lr2 + (lr1 - lr2)*(1 + np.cos(np.pi * i/iters))/2\n",
    "    \n",
    "    def _cosine_schedule(self):\n",
    "        \"\"\"Cosine annealing scheduler. Computes learning rates for each\n",
    "        iteration. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "        \"\"\"\n",
    "        seg1 = self._cosine_anneal(int(self.warm * self.iters), \n",
    "                                   self.min_lr,\n",
    "                                   self.max_lr)\n",
    "        seg2 = self._cosine_anneal(int(np.ceil((1 - self.warm) * self.iters)),\n",
    "                                   self.max_lr,\n",
    "                                   self.min_lr)\n",
    "        return np.concatenate((seg1, seg2))\n",
    "    \n",
    "    def _cosine_restarts_schedule(self):\n",
    "        \"\"\"Cosine annealing with restarts.\"\"\"\n",
    "        cycles = int(np.ceil(self.iters / \n",
    "                             (self.cycle_len * self.iters_per_epoch)))\n",
    "        cycle_iters = self.cycle_len * self.iters_per_epoch\n",
    "        lrs = [self._cosine_anneal(cycle_iters, self.max_lr, self.min_lr)\n",
    "               / (1 + self.cycle_decay * i) for i in range(cycles)]\n",
    "        return np.concatenate(lrs)\n",
    "    \n",
    "    def _schedule(self, restarts):\n",
    "        \"\"\"Wrapper to schedule learning rates depending on chosen method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        restarts: bool\n",
    "            If True, use schedule with restarts. If False, use regular \n",
    "            cosine annealing that spans whole duration of training.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array: LR for each iteration (i.e. output[i] is the LR to use\n",
    "            at iteration i).\n",
    "        \"\"\"\n",
    "        if restarts:\n",
    "            return self._cosine_restarts_schedule()\n",
    "        return self._cosine_schedule()\n",
    "    \n",
    "    def plot_lrs(self, path=None):\n",
    "        \"\"\"Display learning rate by iteration.\n",
    "        \n",
    "        Note: If the plot is not as smooth as expected, this likely \n",
    "        means that there are very few iterations per epoch \n",
    "        (i.e. the batch size is very large, at least in relative terms).\n",
    "        \"\"\"\n",
    "        plt.plot(self.lrs)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        if path:\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the current iteration to zero. Note that if the learning rate\n",
    "        has been decayed, it will now be back at the initial value.\n",
    "        \"\"\"\n",
    "        self.curr_iter = 0\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f'CosineLRScheduler(max_lr={self.max_lr}, '\n",
    "                f'epochs={self.epochs}, '\n",
    "                f'iters_per_epoch={self.iters_per_epoch}, '\n",
    "                f'restarts={self.restarts}, '\n",
    "                f'cycle_len={self.cycle_len}, '\n",
    "                f'min_lr={self.min_lr}, ' \n",
    "                f'verbose={self.verbose})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will use a dummy model on MNIST to show how to use the `CosineLRScheduler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, ReLU, Input, Flatten, GlobalMaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = np.expand_dims(X_train/255., -1), np.expand_dims(X_test/255., -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(24, (3,3), padding='same'),\n",
    "    ReLU(),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(24, (3,3), padding='same'),\n",
    "    ReLU(),\n",
    "    BatchNormalization(),\n",
    "    GlobalMaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BS = 64\n",
    "EPOCHS = 1\n",
    "STEPS_PER_EPOCH = np.ceil(X_train.shape[0]/BS)\n",
    "scheduler = CosineLRScheduler(1e-3, EPOCHS, STEPS_PER_EPOCH)\n",
    "scheduler.plot_lrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "60000/60000 [==============================] - 60s 992us/sample - loss: 1.0316 - accuracy: 0.6771 - val_loss: 0.5077 - val_accuracy: 0.8386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16372d780>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=BS, epochs=EPOCHS, validation_data=(X_test, y_test), callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchHistory(Callback):\n",
    "    \"\"\"```Callback to store training loss at each mini batch. Built in\n",
    "    history callback only stores epoch losses. Validation set is only\n",
    "    evaluated at the end of each epoch so it's not included here.\n",
    "    ```\"\"\"\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        with open(self.path, 'a') as f:\n",
    "            f.write('\\n'.join(str(loss) for loss in self.losses) + '\\n')\n",
    "       \n",
    "    def plot_batch_losses(self, path, smooth=50):\n",
    "        smoothed = [np.mean(self.losses[i:i+smooth]) \n",
    "                    for i in range(len(self.losses))]\n",
    "        \n",
    "        plt.plot(self.losses, lw=.5, c='blue', alpha=.25)\n",
    "        plt.plot(smoothed, lw=.5, c='blue', alpha=1)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Training Loss by Mini Batch.')\n",
    "        if path:\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PerformanceThreshold(Callback):\n",
    "    \"\"\"Callback that will stop training if performance falls short of\n",
    "    some user-specified threshold. This can help avoid wasting time on\n",
    "    runs that have clearly gone wrong - e.g. learning rate is too large\n",
    "    and validation BCE loss has ballooned to >2.0 on a balanced dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric='val_loss', threshold=1.0, goal='min'):\n",
    "        \"\"\"```\n",
    "        Parameters\n",
    "        ----------\n",
    "        metric: str\n",
    "            Name of metric to monitor.\n",
    "        threshold: float\n",
    "            Performance threshold. If the quantity being monitored is\n",
    "            something we want to minimize, this is the highest \n",
    "            acceptable value to continue training. If it's a metric to\n",
    "            maximize like accuracy, this is the minimum acceptable \n",
    "            value.\n",
    "        goal: str\n",
    "            Goal of the quantity being monitored (i.e. are we trying to \n",
    "            minimize metric or maximize it?). One of ('max', 'min').\n",
    "        ```\"\"\"\n",
    "        super().__init__()\n",
    "        assert goal in ('max', 'min')\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.threshold = threshold\n",
    "        if goal == 'max':\n",
    "            self.op = gt\n",
    "        elif goal == 'min':\n",
    "            self.op = lt\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        curr_val = logs.get(self.metric)\n",
    "        if curr_val is not None and self.op(self.threshold, curr_val):\n",
    "            print(f'{self.metric} of {curr_val:.3e} did not satisfy '\n",
    "                  f'performance threshold of {self.threshold}. '\n",
    "                   'Stopping training.')\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_callbacks(prefix, stopping_patience=4, max_loss=1.0,\n",
    "                  monitor='val_loss', tensorboard=False, sched=False, v=3):\n",
    "    \"\"\"```Get a list of callbacks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prefix: str\n",
    "        Name used to identify this run of training. Ex: v2_e3, meaning\n",
    "        MobileNet v2, experiment 3.\n",
    "    stopping_patience: int\n",
    "        Patient parameter for early stopping callback.\n",
    "    max_loss: float\n",
    "        If specified, include a PerformanceThreshold callback with max_loss\n",
    "        as the threshold for validation loss.\n",
    "    tensorboard: bool\n",
    "        Specify whether to use tensorboard callback.\n",
    "    sched: CosineLRScheduler\n",
    "        Learning rate scheduler. Easier to define this outside the function \n",
    "        and pass it in directly (optional).\n",
    "    v: int\n",
    "        Version number. Note that this refers to GoGuardian versioning, not\n",
    "        MobileNet versioning. One of (2, 3), where 2 corresponds to Greg's old\n",
    "        code and 3 is for new models.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, tensorflow.python.keras.callbacks]: Map callback name to callback.\n",
    "        When compiling model, pass in list(callbacks.keys()). Returning a dict \n",
    "        allows us to access specific callbacks more easily, since indices\n",
    "        will vary depending on which callbacks are used.\n",
    "    ```\"\"\"\n",
    "    log_dir = os.path.join(f'logs_v{v}', prefix)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        os.path.join(f'models_v{v}', prefix, 'best_model.h5'),\n",
    "        monitor=monitor, mode='auto', verbose=1, \n",
    "        save_best_only=True, save_weights_only=True\n",
    "    )\n",
    "    stopper = EarlyStopping('val_loss', patience=stopping_patience,\n",
    "                            restore_best_weights=True, verbose=1)\n",
    "    logger = CSVLogger(os.path.join(log_dir, 'logs.csv'), append=True)\n",
    "    batch_hist = BatchHistory(os.path.join(log_dir, 'batch_logs.csv'))\n",
    "    \n",
    "    callbacks = []\n",
    "    if sched:\n",
    "        callbacks.append(sched)\n",
    "    callbacks += [checkpointer, stopper, logger, batch_hist]\n",
    "    if max_loss:\n",
    "        callbacks.append(PerformanceThreshold('val_loss', max_loss, 'min'))\n",
    "    if tensorboard:\n",
    "        callbacks.append(tboard)  \n",
    "    return {cb.__class__.__name__: cb for cb in callbacks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LRFinder:\n",
    "    \"\"\"```\n",
    "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
    "    See for details:\n",
    "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    \n",
    "    The following code is attributed to https://github.com/surmenok/keras_lr_finder\n",
    "    \n",
    "    Usage:\n",
    "    ```\n",
    "    # model is a Keras model\n",
    "    lr_finder = LRFinder(model)\n",
    "\n",
    "    # Train a model with batch size 512 for 5 epochs\n",
    "    # with learning rate growing exponentially from 0.0001 to 1\n",
    "    lr_finder.find(x_train, y_train, start_lr=0.0001, end_lr=1, batch_size=512, epochs=5)\n",
    "    ```\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # Log the learning rate\n",
    "        lr = K.get_value(self.model.optimizer.learning_rate)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # Check whether the loss got too large or NaN\n",
    "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        # Increase the learning rate for the next batch\n",
    "        lr *= self.lr_mult\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "\n",
    "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
    "        # If x_train contains data for multiple inputs, use length of the first input.\n",
    "        # Assumption: the first element in the list is single input; NOT a list of inputs.\n",
    "        N = x_train[0].shape[0] if isinstance(x_train, list) else x_train.shape[0]\n",
    "\n",
    "        # Compute number of batches and LR multiplier\n",
    "        num_batches = epochs * N / batch_size\n",
    "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.learning_rate)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.learning_rate, start_lr)\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit(x_train, y_train,\n",
    "                       batch_size=batch_size, epochs=epochs,\n",
    "                       callbacks=[callback])\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        K.set_value(self.model.optimizer.learning_rate, original_lr)\n",
    "\n",
    "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
    "        if steps_per_epoch is None:\n",
    "            try:\n",
    "                steps_per_epoch = len(generator)\n",
    "            except (ValueError, NotImplementedError) as e:\n",
    "                raise e('`steps_per_epoch=None` is only valid for a'\n",
    "                        ' generator based on the '\n",
    "                        '`keras.utils.Sequence`'\n",
    "                        ' class. Please specify `steps_per_epoch` '\n",
    "                        'or use the `keras.utils.Sequence` class.')\n",
    "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(epochs * steps_per_epoch))\n",
    "\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.learning_rate)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.learning_rate, start_lr)\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
    "                                                      logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit_generator(generator=generator,\n",
    "                                 epochs=epochs,\n",
    "                                 steps_per_epoch=steps_per_epoch,\n",
    "                                 callbacks=[callback],\n",
    "                                 **kw_fit)\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        K.set_value(self.model.optimizer.learning_rate, original_lr)\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
    "        \"\"\"\n",
    "        Plots the loss.\n",
    "        Parameters:\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "        \"\"\"\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale(x_scale)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        \"\"\"\n",
    "        Plots rate of change of the loss function.\n",
    "        Parameters:\n",
    "            sma - number of batches for simple moving average to smooth out the curve.\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "            y_lim - limits for the y axis.\n",
    "        \"\"\"\n",
    "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
    "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(lrs, derivatives)\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)\n",
    "        plt.show()\n",
    "\n",
    "    def get_derivatives(self, sma):\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
    "        return derivatives\n",
    "\n",
    "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
    "        derivatives = self.get_derivatives(sma)\n",
    "        best_der_idx = np.argmax(derivatives[n_skip_beginning:-n_skip_end])[0]\n",
    "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "get_checkpoint_callback = partial(ModelCheckpoint, \n",
    "                                  monitor='val_loss', \n",
    "                                  verbose=True,\n",
    "                                  save_best_only=True, \n",
    "                                  save_weights_only=True)\n",
    "\n",
    "tboard = TensorBoard(os.path.join('logs', 'tboard'), \n",
    "                     histogram_freq=1,\n",
    "                     write_images=True)\n",
    "\n",
    "early_stopper = EarlyStopping('val_loss', patience=4, \n",
    "                              restore_best_weights=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
