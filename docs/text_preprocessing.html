---

title: Text Preprocessing

keywords: fastai
sidebar: home_sidebar

summary: "Module for preprocessing text. "
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02_text_preprocessing.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="spec_add_spaces" class="doc_header"><code>spec_add_spaces</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L45" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>spec_add_spaces</code>(<strong><code>t</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Add spaces around / and # in <code>t</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="separate_punctuation" class="doc_header"><code>separate_punctuation</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L49" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>separate_punctuation</code>(<strong><code>string</code></strong>)</p>
</blockquote>
<p>Adds spaces around all punctuation.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="rm_useless_spaces" class="doc_header"><code>rm_useless_spaces</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L56" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>rm_useless_spaces</code>(<strong><code>t</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Remove multiple spaces in <code>t</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="replace_rep" class="doc_header"><code>replace_rep</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L60" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>replace_rep</code>(<strong><code>string</code></strong>)</p>
</blockquote>
<p>Replace repetitions at the character level in <code>string</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="replace_wrep" class="doc_header"><code>replace_wrep</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L68" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>replace_wrep</code>(<strong><code>t</code></strong>)</p>
</blockquote>
<p>Replace word repetitions in <code>t</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="fix_html" class="doc_header"><code>fix_html</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L76" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>fix_html</code>(<strong><code>x</code></strong>)</p>
</blockquote>
<p>List of replacements from html strings in <code>x</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="replace_all_caps" class="doc_header"><code>replace_all_caps</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L86" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>replace_all_caps</code>(<strong><code>x</code></strong>)</p>
</blockquote>
<p>Replace tokens in ALL CAPS in <code>x</code> by their lower version and add <a href="text.preprocessing.html#TK_UP"><code>TK_UP</code></a> before.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="deal_caps" class="doc_header"><code>deal_caps</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L94" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>deal_caps</code>(<strong><code>x</code></strong>)</p>
</blockquote>
<p>Replace all Capitalized tokens in <code>x</code> by their lower version and add <a href="text.preprocessing.html#TK_MAJ"><code>TK_MAJ</code></a> before.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="fix_ascii" class="doc_header"><code>fix_ascii</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L103" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>fix_ascii</code>(<strong><code>string</code></strong>)</p>
</blockquote>
<p>Fixes any ascii characters that may be in the text.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="preprocess_text" class="doc_header"><code>preprocess_text</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L111" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>preprocess_text</code>(<strong><code>string</code></strong>, <strong><code>funcs</code></strong>=<em><code>[&lt;function fix_html at 0x1465a2268&gt;, &lt;function replace_rep at 0x1465a2158&gt;, &lt;function replace_wrep at 0x1465a21e0&gt;, &lt;function separate_punctuation at 0x14658ba60&gt;, &lt;function rm_useless_spaces at 0x1465a20d0&gt;, &lt;function replace_all_caps at 0x1465a22f0&gt;, &lt;function deal_caps at 0x1465a2378&gt;]</code></em>)</p>
</blockquote>
<p>General function for cleaning text.
Applites functions in order to a string.</p>
<p>Options:</p>
<p>string: string you want to process for tokenization.
funcs: list, series of ordered functions you'd like to apply to string.</p>
<p>Returns:</p>
<p>string, text that's been processed by funcs.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These functions help clean text. They are largely taken from fastai.text.transform. They perform some basic text cleaning functionality.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocess_text</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">Hello!</span><span class="se">\n</span><span class="s1">This is an example of how to #clean text up a bit...</span>
<span class="s1">aaaaa AAA</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;hello ! \n this is an example of how to # clean text up a bit . . . \n xxrep 5 a aaa&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BaseTokenizer" class="doc_header"><code>class</code> <code>BaseTokenizer</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L128" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BaseTokenizer</code>(<strong><code>lang</code></strong>)</p>
</blockquote>
<p>Basic class for a tokenizer function.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SpacyTokenizer" class="doc_header"><code>class</code> <code>SpacyTokenizer</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L137" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SpacyTokenizer</code>(<strong><code>lang</code></strong>) :: <a href="text.preprocessing.html#BaseTokenizer"><code>BaseTokenizer</code></a></p>
</blockquote>
<p>Wrapper around a spacy tokenizer to make it a <a href="text.preprocessing.html#BaseTokenizer"><code>BaseTokenizer</code></a>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SentencePieceTokenizer" class="doc_header"><code>class</code> <code>SentencePieceTokenizer</code><a href="https://github.com/mgfrantz/AugmenTF/tree/master/AugmenTF/text/text_preprocessing.py#L149" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SentencePieceTokenizer</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def get_default_size(texts, max_vocab_sz):</span>
<span class="c1">#     &quot;Either max_vocab_sz or one quarter of the number of unique words in `texts`&quot;</span>
<span class="c1">#     cnt = Counter()</span>
<span class="c1">#     for t in texts: </span>
<span class="c1">#         cnt.update(t.split())</span>
<span class="c1">#         if len(cnt)//4 &gt; max_vocab_sz: return max_vocab_sz</span>
<span class="c1">#     res = len(cnt)//4</span>
<span class="c1">#     while res%8 != 0: res+=1</span>
<span class="c1">#     return res</span>

<span class="c1"># def train_sentencepiece(texts:Collection[str], path:PathOrStr, pre_rules: ListRules=None, post_rules:ListRules=None, </span>
<span class="c1">#     vocab_sz:int=None, max_vocab_sz:int=30000, model_type:str=&#39;unigram&#39;, max_sentence_len:int=20480, lang=&#39;en&#39;,</span>
<span class="c1">#     char_coverage=None, tmp_dir=&#39;tmp&#39;, enc=&#39;utf8&#39;):</span>
<span class="c1">#     &quot;Train a sentencepiece tokenizer on `texts` and save it in `path/tmp_dir`&quot;</span>
<span class="c1">#     from sentencepiece import SentencePieceTrainer</span>
<span class="c1">#     cache_dir = Path(path)/tmp_dir</span>
<span class="c1">#     os.makedirs(cache_dir, exist_ok=True)</span>
<span class="c1">#     if vocab_sz is None: vocab_sz=get_default_size(texts, max_vocab_sz)</span>
<span class="c1">#     raw_text_path = cache_dir / &#39;all_text.out&#39;</span>
<span class="c1">#     with open(raw_text_path, &#39;w&#39;, encoding=enc) as f: f.write(&quot;\n&quot;.join(texts))</span>
<span class="c1">#     spec_tokens = [&#39;\u2581&#39;+s for s in defaults.text_spec_tok]</span>
<span class="c1">#     cache_dir = cache_dir/&#39;spm&#39;</span>
<span class="c1">#     SentencePieceTrainer.Train(&quot; &quot;.join([</span>
<span class="c1">#         f&quot;--input={raw_text_path} --max_sentence_length={max_sentence_len}&quot;,</span>
<span class="c1">#         f&quot;--character_coverage={ifnone(char_coverage, 0.99999 if lang in full_char_coverage_langs else 0.9998)}&quot;,</span>
<span class="c1">#         f&quot;--unk_id={len(defaults.text_spec_tok)} --pad_id=-1 --bos_id=-1 --eos_id=-1&quot;,</span>
<span class="c1">#         f&quot;--user_defined_symbols={&#39;,&#39;.join(spec_tokens)}&quot;,</span>
<span class="c1">#         f&#39;--model_prefix=&quot;cache_dir&quot; --vocab_size={vocab_sz} --model_type={model_type}&#39;]))</span>
<span class="c1">#     raw_text_path.unlink()</span>
<span class="c1">#     return cache_dir</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>
 

