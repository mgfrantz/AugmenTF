{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dask_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Datasets\n",
    "\n",
    "> Module to build tensorflow datasets from Dask DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from dask import dataframe as dd\n",
    "import pandas as pd\n",
    "from AugmenTF.text.text_preprocessing import preprocess_text\n",
    "from AugmenTF.core import *\n",
    "from random import shuffle\n",
    "import sys\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "class IdxMapper():\n",
    "    def __init__(self):\n",
    "        self.idx=0\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        r = list(range(self.idx, self.idx + df.index.size))\n",
    "        self.idx += df.index.size\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def ddf_make_index_monotonic(ddf, persist=True):\n",
    "    ddf['idx'] = 1\n",
    "    ddf['idx'] = ddf.idx.cumsum() - 1\n",
    "    ddf = ddf.set_index('idx', sorted=True)\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def cull_empty_partitions(df):\n",
    "    ll = list(df.map_partitions(len).compute())\n",
    "    df_delayed = df.to_delayed()\n",
    "    df_delayed_new = list()\n",
    "    pempty = None\n",
    "    for ix, n in enumerate(ll):\n",
    "        if 0 == n:\n",
    "            pempty = df.get_partition(ix)\n",
    "        else:\n",
    "            df_delayed_new.append(df_delayed[ix])\n",
    "    if pempty is not None:\n",
    "        df = dd.from_delayed(df_delayed_new, meta=pempty)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "TYPE_MAPPING = {\n",
    "    'str':tf.string,\n",
    "    'object':tf.string,\n",
    "    'int':tf.int32,\n",
    "    'float':tf.float32,\n",
    "    'cat':tf.int32\n",
    "}\n",
    "\n",
    "_all_ = ['TYPE_MAPPING']\n",
    "\n",
    "def _map_dtypes(df):\n",
    "    '''```\n",
    "    Returns a dictionary of \n",
    "    ```'''\n",
    "    type_dict = {\n",
    "        col:str(t)\n",
    "        for col, t\n",
    "        in df.dtypes.to_dict().items()\n",
    "    }\n",
    "    for k, v in type_dict.items():\n",
    "        for tpe, tf_type in TYPE_MAPPING.items():\n",
    "            if tpe in v:\n",
    "                type_dict[k] = tf_type\n",
    "    return type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DaskDataset():\n",
    "    def __init__(self, df):\n",
    "        '''\n",
    "        Class to manage Dask dataframes as Tensorflow datasets.\n",
    "        \n",
    "        \n",
    "        df: any dask.DataFrame\n",
    "        \n",
    "        '''\n",
    "        assert isinstance(df, dd.DataFrame), '''\n",
    "        df must be a dask.dataframe.DataFrame\n",
    "        '''\n",
    "        self.df = ddf_make_index_monotonic(df)\n",
    "        self.n_rows = df.shape[0].compute()\n",
    "        self.n_cols = len(df.columns)\n",
    "        self.shape = (self.n_rows, self.n_cols)\n",
    "        self.index = self.df.index.compute().tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _idx = self.index[idx]\n",
    "        return self.df.loc[_idx].compute()\n",
    "    \n",
    "    def select_columns(self, x_cols=None, y_cols=None):\n",
    "        '''\n",
    "        Select which columns you want as your X and y.\n",
    "        '''\n",
    "        self.x_cols = x_cols\n",
    "        self.y_cols = y_cols\n",
    "    \n",
    "    def to_parquet(self, path, partition_cols=None):\n",
    "        \"\"\"```\n",
    "        Saves the self.df as parquet.\n",
    "        \n",
    "        Options:\n",
    "        path: str or Path, path to where you want the saved data to live.\n",
    "        partition_cols: list of columns on which to partition the data.\n",
    "        ```\"\"\"\n",
    "        \n",
    "        path = Path(path)\n",
    "        if not path.parent.exists():\n",
    "            path.parent.mkdir(parents=True)\n",
    "        if partition_cols and isinstance(partition_cols, str):\n",
    "            partition_cols = [partition_cols]\n",
    "        self.df.to_parquet(str(path), partition_on=partition_cols, write_index=False)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}\\n' +\n",
    "            f'Columns and Datatypes\\n' +\n",
    "            f'{self.df.dtypes}\\n'  +\n",
    "            f'Shape: {self.shape}'\n",
    "        )\n",
    "    \n",
    "    def get_tf_dataset(self, randomize_order=True):\n",
    "        '''\n",
    "        Returns a tf.data.Dataset of the data.\n",
    "        Must have x_cols and y_cols selected using DaskDataset.select_columns\n",
    "        '''\n",
    "        \n",
    "        # confirm that x and y have been set\n",
    "        if (not hasattr(self, 'x_cols')) or (not hasattr(self, 'y_cols')):\n",
    "            raise ValueError('Must use select_columns to select data and labels')\n",
    "        \n",
    "        # Create a generator from iterrows\n",
    "        def gen():\n",
    "            for i, row in self.df.iterrows():\n",
    "                if isinstance(self.x_cols, (list,tuple)):\n",
    "                    x = row[self.x_cols].values\n",
    "                else:\n",
    "                    x = row[self.x_cols]\n",
    "                if isinstance(self.y_cols, (list,tuple)):\n",
    "                    y = row[self.y_cols].values\n",
    "                else:\n",
    "                    y = row[self.y_cols]\n",
    "            yield x,y\n",
    "        \n",
    "        # Get the datatypes in the right format\n",
    "        _types = _map_dtypes(self.df)\n",
    "        if isinstance(self.x_cols, (list, tuple)):\n",
    "            x_types = (_types[col] for col in self.x_cols)\n",
    "        else:\n",
    "            x_types = _types[self.x_cols]\n",
    "        if isinstance(self.y_cols, (list, tuple)):\n",
    "            y_types = (_types[col] for col in self.y_cols)\n",
    "        else:\n",
    "            y_types = _types[self.y_cols]\n",
    "        \n",
    "        \n",
    "        return tf.data.Dataset.from_generator(gen, (x_types, y_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_random_splits(df, train_frac=0.8, val_frac=0.1, test_frac=0.1, stratify_by=None, random_state=None):\n",
    "    '''```\n",
    "    Splits your data into train, validation, and test sets.\n",
    "    This function also supports stratification by one column.\n",
    "    \n",
    "    Options:\n",
    "    df: any dask.DataFrame\n",
    "    train_frac: float, fraction of data you'd like to be in your train df\n",
    "    val_frac: float, fraction of data you'd like to be in your validation df\n",
    "    test_frac: float, fraction of data you'd like to be in your test df.\n",
    "               if None, will only return train and val datasets.\n",
    "    stratify_by: str, col you'd like to use to stratify your splits\n",
    "    random_state: int or None, random state for your splits\n",
    "    \n",
    "    Returns: tuple of DataFrames.\n",
    "    ```'''\n",
    "\n",
    "    assert isinstance(df, dd.DataFrame), '''\n",
    "    df myst be a dask.dataframe.DataFrame\n",
    "    '''\n",
    "\n",
    "    if stratify_by:\n",
    "        assert stratify_by in df.columns\n",
    "\n",
    "    fracs = [frac for frac in [train_frac, val_frac, test_frac] if frac]\n",
    "    assert sum(fracs) == 1, '''\n",
    "    train_frac, val_frac, and test_frac must sum to 1.0\n",
    "    '''\n",
    "\n",
    "    df_lists = [[],[],[]]\n",
    "    if not stratify_by:\n",
    "        return [\n",
    "            _df.reset_index(drop=True)\n",
    "            for _df\n",
    "            in df.random_split(fracs, random_state=random_state)\n",
    "        ]\n",
    "    else:\n",
    "        df_lists = [[],[],[]]\n",
    "        values = df[stratify_by].unique().compute().tolist()\n",
    "        gb = df.groupby(stratify_by)\n",
    "        for v in values:\n",
    "            _df = gb.get_group(v)\n",
    "            _split = _df.random_split(fracs, random_state=random_state)\n",
    "            for i, _sdf in enumerate(_split):\n",
    "                df_lists[i].append(_sdf)\n",
    "        for i,l in enumerate(df_lists):\n",
    "            if l:\n",
    "                df_lists[i] = dd.concat(l, interleave_partitions=True).reset_index(drop=True)\n",
    "            else:\n",
    "                del df_lists[i]\n",
    "        return tuple(df_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll illustrate a basic workflow for making `tf.data.Dataset`s from `DaskDataset`s, including loading and splitting data, and making transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AugmenTF.text.text_preprocessing import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load data into a `DataFrame`. Dataframes can also be read from CSVs, Parquet, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "             label_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/csv/20_newsgroups.csv')\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate a simple transformation, we'll preprocess the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.text = ddf.text.map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from : lerxst @ wam . umd . edu ( where ' s my...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from : guykuo @ carson . u . washington . edu ...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from : twillis @ ec . ecn . purdue . edu ( tho...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from : jgreen @ amber ( joe green ) \\n subject...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from : jcm @ head - cfa . harvard . edu ( jona...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels  \\\n",
       "0  from : lerxst @ wam . umd . edu ( where ' s my...       7   \n",
       "1  from : guykuo @ carson . u . washington . edu ...       4   \n",
       "2  from : twillis @ ec . ecn . purdue . edu ( tho...       4   \n",
       "3  from : jgreen @ amber ( joe green ) \\n subject...       1   \n",
       "4  from : jcm @ head - cfa . harvard . edu ( jona...      14   \n",
       "\n",
       "             label_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll make one train/val split and stratify by the `\"label_names\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = make_random_splits(ddf, .8, .2, None, 'label_names', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we make `DaskDataset`s out of the train and val DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = DaskDataset(train), DaskDataset(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily index `DaskDatasets` since they enforce monotonically increasing indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>from : speedy @ engr . latech . edu ( speedy m...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels  \\\n",
       "idx                                                               \n",
       "1000  from : speedy @ engr . latech . edu ( speedy m...       8   \n",
       "\n",
       "          label_names  \n",
       "idx                    \n",
       "1000  rec.motorcycles  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to create a tensorflow Dataset, we have to set the X and y columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.select_columns(x_cols='text', y_cols='labels')\n",
    "val_ds.select_columns(x_cols='text', y_cols='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can easily get tensorflow datasets from the train or val datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = train_ds.get_tf_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch, y_batch in tf_train.batch(10):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=\n",
       "array([b'from : rdippold @ qualcomm . com ( ron \" asbestos \" dippold ) \\n subject : re : once tapped , your code is no good any more . \\n originator : rdippold @ qualcom . qualcomm . com \\n nntp - posting - host : qualcom . qualcomm . com \\n organization : qualcomm , inc . , san diego , ca \\n distribution : na \\n lines : 8 \\n \\n random @ presto . uucp ( jeff w . hyche ) writes : \\n > yes , \" clipper \" is a trademark of intergraph . its the risc chip used \\n > in some of thier workstations . i wonder what intergraph is going to \\n > do to this infringement on thier name sake ? \\n \\n probably keep quiet and take it , lest they get their kneecaps busted . \\n - - \\n good news . ten weeks from friday will be a good day .'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* regardless of batch size, tf dataset only returns batches of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
