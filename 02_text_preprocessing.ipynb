{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp text.text_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "> Module for preprocessing text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from AugmenTF.core import *\n",
    "from AugmenTF.basic_data import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow import strings, py_function\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import html\n",
    "from string import punctuation\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import sentencepiece as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "_all_ = [\n",
    "    \"BOS\",\n",
    "    \"EOS\",\n",
    "    \"FLD\",\n",
    "    \"UNK\",\n",
    "    \"PAD\",\n",
    "    \"TK_MAJ\",\n",
    "    \"TK_UP\",\n",
    "    \"TK_REP\",\n",
    "    \"TK_WREP\",\n",
    "    \"STEPS\",\n",
    "]\n",
    "\n",
    "BOS = 'xxbos'\n",
    "EOS = 'xxeos'\n",
    "FLD = 'xxfld'\n",
    "UNK = 'xxunk'\n",
    "PAD = 'xxpad'\n",
    "TK_MAJ = 'xxmaj'\n",
    "TK_UP = 'xxup'\n",
    "TK_REP = 'xxrep'\n",
    "TK_WREP = 'xxwrep'\n",
    "\n",
    "def spec_add_spaces(t:str) -> str:\n",
    "    \"Add spaces around / and # in `t`. \\n\"\n",
    "    return re.sub(r'([/#\\n])', r' \\1 ', t)\n",
    "\n",
    "def separate_punctuation(string):\n",
    "    '''\n",
    "    Adds spaces around all punctuation.\n",
    "    '''\n",
    "    return re.sub(f'([{punctuation}\\n])', r' \\1 ',\n",
    "                  string).strip()\n",
    "\n",
    "def rm_useless_spaces(t:str) -> str:\n",
    "    \"Remove multiple spaces in `t`.\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(string):\n",
    "    \"Replace repetitions at the character level in `string`.\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, string)\n",
    "\n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions in `t`.\"\n",
    "    def _replace_wrep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fix_html(x):\n",
    "    \"List of replacements from html strings in `x`.\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    \n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace(' @,@ ',',').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS in `x` by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return ''.join(res)\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in `x` by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return ''.join(res)\n",
    "\n",
    "def fix_ascii(string):\n",
    "    '''\n",
    "    Fixes any ascii characters that may be in the text.\n",
    "    '''\n",
    "    return string.encode('ascii', errors='ignore').decode('utf-8')\n",
    "\n",
    "STEPS = [fix_html, replace_rep, replace_wrep, separate_punctuation, rm_useless_spaces, replace_all_caps, deal_caps]\n",
    "\n",
    "def preprocess_text(string, funcs=STEPS):\n",
    "    '''\n",
    "    General function for cleaning text.\n",
    "    Applites functions in order to a string.\n",
    "    \n",
    "    Options:\n",
    "    \n",
    "    string: string you want to process for tokenization.\n",
    "    funcs: list, series of ordered functions you'd like to apply to string.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    string, text that's been processed by funcs.\n",
    "    '''\n",
    "    return apply_chained_funcs(string, funcs).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions help clean text. They are largely taken from fastai.text.transform. They perform some basic text cleaning functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello ! \\n this is an example of how to # clean text up a bit . . . \\n xxrep 5 a aaa'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text('''\n",
    "Hello!\\nThis is an example of how to #clean text up a bit...\n",
    "aaaaa AAA\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "class BaseTokenizer():\n",
    "    \"Basic class for a tokenizer function.\"\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "    def tokenizer(self, t):\n",
    "        return t.split(' ')\n",
    "    def add_special_cases(self, toks):\n",
    "        pass\n",
    "\n",
    "class SpacyTokenizer(BaseTokenizer):\n",
    "    \"Wrapper around a spacy tokenizer to make it a `BaseTokenizer`.\"\n",
    "    def __init__(self, lang):\n",
    "        self.tok = spacy.blank(lang, disable=[\"parser\",\"tagger\",\"ner\"])\n",
    "\n",
    "    def tokenizer(self, t):\n",
    "        return [t.text for t in self.tok.tokenizer(t)]\n",
    "\n",
    "    def add_special_cases(self, tokens):\n",
    "        for w in tokens:\n",
    "            self.tok.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "            \n",
    "class SentencePieceTokenizer():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SentencePieceTrainer_Train() takes exactly 1 argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-730168baec58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: SentencePieceTrainer_Train() takes exactly 1 argument (0 given)"
     ]
    }
   ],
   "source": [
    "trainer = sps.SentencePieceTrainer.Train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blockers:\n",
    "* regex does not support backreferences. This prevents implementation of some core functionality as a tf.function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
